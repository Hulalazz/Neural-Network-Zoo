{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Neural Algorithm of Artistic Style\n",
    "This is a jupyter notebook implementing [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576). This implementation takes elements from [Improving the Neural Algorithm of Artistic Style](http://arxiv.org/abs/1605.04603) and is an extensions of [@titu1994](https://github.com/titu1994)'s [implementation](https://github.com/titu1994/Neural-Style-Transfer/blob/master/INetwork.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imread, imresize, imsave, fromimage, toimage\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, AveragePooling2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils.layer_utils import convert_all_kernels_in_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Verify that the backend matches the image dimension ordering.\n",
    "if K.backend() == 'tensorflow' and K.image_dim_ordering() == \"th\":\n",
    "    # TODO package this into a tools file.\n",
    "    warnings.warn('You are using the TensorFlow backend, yet you '\n",
    "                  'are using the Theano '\n",
    "                  'image dimension ordering convention '\n",
    "                  '(`image_dim_ordering=\"th\"`). '\n",
    "                  'For best performance, set '\n",
    "                  '`image_dim_ordering=\"tf\"` in '\n",
    "                  'your Keras config '\n",
    "                  'at ~/.keras/keras.json.')\n",
    "    convert_all_kernels_in_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_image_path = 'landscape.jpg'# Path to the image to transform.\n",
    "style_image_paths = ['vangogh.jpg']# Path to the style reference image[s].\n",
    "result_prefix = 'style'# Prefix for the saved results.\n",
    "\n",
    "# Mask setup\n",
    "mask_paths = [] # Masks for style images\n",
    "color_mask = None # Leave as None, or specify a mask\n",
    "\n",
    "# Output Image settings\n",
    "img_width = img_height = 400 # Change the side length. Grammian just restricts to square images.\n",
    "preserve_color = False # Make true if you want to preserv the color of the content image. \n",
    "\n",
    "# Content weighting\n",
    "content_weight = 0.025 # How much you want the content to affect the total cost\n",
    "style_weights = [1] # weights of different styles\n",
    "style_scale = 1 # Different style scaling\n",
    "total_variation_weight = 8.5e-5 # Total Variation in the Weights\n",
    "\n",
    "# Training variables\n",
    "num_iter = 10 # Number of iterations to go through.\n",
    "vgg_model = 'vgg16' # The ImageNet model you want to train and use. choices are 'vgg16' and 'vgg19'\n",
    "content_loss_type = 0 # Can be 0, 1, or 2. TODO README.\n",
    "rescale_image = False # Rescale image after execution to original dimentions\n",
    "rescale_method = 'bilinear' # Image rescaling algorithm. nearest’, ‘lanczos’, ‘bilinear’, ‘bicubic’ or ‘cubic’.\n",
    "maintain_aspect_ratio = True\n",
    "content_layer = \"conv5_2\" # Specify what layer contains the content of the image. Check feature_layers for options\n",
    "init_image=\"content\" # Initial image used to generate the final image. Options are 'content', 'noise', or 'gray'\n",
    "pooltype = \"max\" # Pooling type. Can be \"ave\" for average pooling or \"max\" for max pooling \n",
    "\n",
    "improvement_threshold = 0.0 # The threshold that dicates when to stop iterating. Keep at 0 to finish all iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Argument checking\n",
    "Run this cell to make sure your arguments are legal and perform necessary preprocessing. IF you don't run this cell, your program will fail or act unexpectedly.\n",
    "\n",
    "You should only be worried if errors are thrown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "style_masks_present = len(mask_paths) > 0\n",
    "if style_masks_present:\n",
    "    assert len(style_image_paths) == len(mask_paths), \"Wrong number of style masks provided.\\n\" \\\n",
    "                                                               \"Number of style images = %d, \\n\" \\\n",
    "                                                               \"Number of style mask paths = %d.\" % \\\n",
    "                                                               (len(style_image_paths), len(style_masks_present))\n",
    "if len(style_image_paths) != len(style_weights):\n",
    "    print(\"Mismatch in number of style images provided and number of style weights provided. \\n\"\n",
    "          \"Found %d style images and %d style weights. \\n\"\n",
    "          \"Equally distributing weights to all styles.\" % (len(style_image_paths), len(style_weights)))\n",
    "\n",
    "    weight_sum = sum(style_weights) * style_scale\n",
    "    count = len(style_image_paths)\n",
    "    style_weights = [weight_sum / count] * count\n",
    "else:\n",
    "    style_weights = [w * style_scale for w in style_weights]\n",
    "\n",
    "assert vgg_model in ('vgg16', 'vgg19'), \"You must choose either 'vgg16' or 'vgg19' as your model type.\"\n",
    "assert content_loss_type in [0, 1, 2], \"Content Loss Type must be one of 0, 1 or 2\"\n",
    "assert init_image in [\"content\", \"noise\", \"gray\"], \"init_image must be one of ['content', 'noise', 'gray']\"\n",
    "\n",
    "assert pooltype in [\"ave\", \"max\"], 'Pooling argument is wrong. Needs to be either \"ave\" or \"max\".'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO deal with this.\n",
    "img_WIDTH = img_HEIGHT = 0\n",
    "aspect_ratio = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Processing Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# util function to open, resize and format pictures into appropriate tensors\n",
    "def preprocess_image(image_path, load_dims=False, read_mode=\"color\"):\n",
    "    global img_WIDTH, img_HEIGHT, aspect_ratio\n",
    "\n",
    "    mode = \"RGB\" if read_mode == \"color\" else \"L\"\n",
    "    img = imread(image_path, mode=mode)  # Prevents crashes due to PNG images (ARGB)\n",
    "\n",
    "    if mode == \"L\":\n",
    "        # Expand the 1 channel grayscale to 3 channel grayscale image\n",
    "        temp = np.zeros(img.shape + (3,), dtype=np.uint8)\n",
    "        temp[:, :, 0] = img\n",
    "        temp[:, :, 1] = img.copy()\n",
    "        temp[:, :, 2] = img.copy()\n",
    "\n",
    "        img = temp\n",
    "\n",
    "    if load_dims:\n",
    "        img_WIDTH = img.shape[0]\n",
    "        img_HEIGHT = img.shape[1]\n",
    "        aspect_ratio = img_HEIGHT / img_WIDTH\n",
    "\n",
    "    img = imresize(img, (img_width, img_height)).astype('float32')\n",
    "\n",
    "    # RGB -> BGR\n",
    "    img = img[:, :, ::-1]\n",
    "\n",
    "    img[:, :, 0] -= 103.939\n",
    "    img[:, :, 1] -= 116.779\n",
    "    img[:, :, 2] -= 123.68\n",
    "\n",
    "    if K.image_dim_ordering() == \"th\":\n",
    "        img = img.transpose((2, 0, 1)).astype('float32')\n",
    "\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "\n",
    "# util function to convert a tensor into a valid image\n",
    "def deprocess_image(x):\n",
    "    if K.image_dim_ordering() == \"th\":\n",
    "        x = x.reshape((3, img_width, img_height))\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    else:\n",
    "        x = x.reshape((img_width, img_height, 3))\n",
    "\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "\n",
    "    # BGR -> RGB\n",
    "    x = x[:, :, ::-1]\n",
    "\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# util function to preserve image color\n",
    "def original_color_transform(content, generated, mask=None):\n",
    "    generated = fromimage(toimage(generated, mode='RGB'), mode='YCbCr')  # Convert to YCbCr color space\n",
    "\n",
    "    if mask is None:\n",
    "        generated[:, :, 1:] = content[:, :, 1:]  # Generated CbCr = Content CbCr\n",
    "    else:\n",
    "        width, height, channels = generated.shape\n",
    "\n",
    "        for i in range(width):\n",
    "            for j in range(height):\n",
    "                if mask[i, j] == 1:\n",
    "                    generated[i, j, 1:] = content[i, j, 1:]\n",
    "\n",
    "    generated = fromimage(toimage(generated, mode='YCbCr'), mode='RGB')  # Convert to RGB color space\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mask(mask_path, shape, return_mask_img=False):\n",
    "    if K.image_dim_ordering() == \"th\":\n",
    "        _, channels, width, height = shape\n",
    "    else:\n",
    "        _, width, height, channels = shape\n",
    "\n",
    "    mask = imread(mask_path, mode=\"L\") # Grayscale mask load\n",
    "    mask = imresize(mask, (width, height)).astype('float32')\n",
    "\n",
    "    # Perform binarization of mask\n",
    "    mask[mask <= 127] = 0\n",
    "    mask[mask > 128] = 255\n",
    "\n",
    "    max = np.amax(mask)\n",
    "    mask /= max\n",
    "\n",
    "    if return_mask_img: return mask\n",
    "\n",
    "    mask_shape = shape[1:]\n",
    "\n",
    "    mask_tensor = np.empty(mask_shape)\n",
    "\n",
    "    for i in range(channels):\n",
    "        if K.image_dim_ordering() == \"th\":\n",
    "            mask_tensor[i, :, :] = mask\n",
    "        else:\n",
    "            mask_tensor[:, :, i] = mask\n",
    "\n",
    "    return mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Decide pooling function\n",
    "def pooling_func():\n",
    "    if pooltype == \"ave\":\n",
    "        return AveragePooling2D((2, 2), strides=(2, 2))\n",
    "    else:\n",
    "        return MaxPooling2D((2, 2), strides=(2, 2))\n",
    "\n",
    "\n",
    "read_mode = \"gray\" if init_image == \"gray\" else \"color\"\n",
    "# get tensor representations of our images\n",
    "base_image = K.variable(preprocess_image(base_image_path, True, read_mode=read_mode))\n",
    "\n",
    "style_reference_images = []\n",
    "for style_path in style_image_paths:\n",
    "    style_reference_images.append(K.variable(preprocess_image(style_path)))\n",
    "\n",
    "# this will contain our generated image\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    combination_image = K.placeholder((1, 3, img_width, img_height))\n",
    "else:\n",
    "    combination_image = K.placeholder((1, img_width, img_height, 3))\n",
    "\n",
    "image_tensors = [base_image]\n",
    "for style_image_tensor in style_reference_images:\n",
    "    image_tensors.append(style_image_tensor)\n",
    "image_tensors.append(combination_image)\n",
    "\n",
    "nb_tensors = len(image_tensors)\n",
    "nb_style_images = nb_tensors - 2 # Content and Output image not considered\n",
    "\n",
    "# combine the various images into a single Keras tensor\n",
    "input_tensor = K.concatenate(image_tensors, axis=0)\n",
    "\n",
    "if K.image_dim_ordering() == \"th\":\n",
    "    shape = (nb_tensors, 3, img_width, img_height)\n",
    "else:\n",
    "    shape = (nb_tensors, img_width, img_height, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the VGG16 network with our 3 images as input\n",
    "first_layer = Convolution2D(64, 3, 3, activation='relu', name='conv1_1', border_mode='same')\n",
    "first_layer.set_input(input_tensor, shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(first_layer)\n",
    "model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2', border_mode='same'))\n",
    "model.add(pooling_func())\n",
    "\n",
    "model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1', border_mode='same'))\n",
    "model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2', border_mode='same'))\n",
    "model.add(pooling_func())\n",
    "\n",
    "model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1', border_mode='same'))\n",
    "model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2', border_mode='same'))\n",
    "model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3', border_mode='same'))\n",
    "if vgg_model == \"vgg19\":\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_4', border_mode='same'))\n",
    "model.add(pooling_func())\n",
    "\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1', border_mode='same'))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2', border_mode='same'))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3', border_mode='same'))\n",
    "if vgg_model == \"vgg19\":\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_4', border_mode='same'))\n",
    "model.add(pooling_func())\n",
    "\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1', border_mode='same'))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2', border_mode='same'))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3', border_mode='same'))\n",
    "if vgg_model == \"vgg19\":\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_4', border_mode='same'))\n",
    "model.add(pooling_func())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_origins = {\n",
    "    ('vgg19', 'th') : 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_th_dim_ordering_th_kernels_notop.h5',\n",
    "    ('vgg19', 'tf') : 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "    ('vgg16', 'th') : 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_th_dim_ordering_th_kernels_notop.h5',\n",
    "    ('vgg16', 'tf') : 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# load VGG weights\n",
    "weights_tuple = (vgg_model, K.image_dim_ordering())\n",
    "\n",
    "filename = '{0}_weights_{1}_dim_ordering_{1}_kernels_notop.h5'.format(*weights_tuple)\n",
    "weights = get_file(filename, weights_origins[weights_tuple], cache_subdir='models')\n",
    "model.load_weights(weights)\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "shape_dict = dict([(layer.name, layer.output_shape) for layer in model.layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the neural style loss\n",
    "First we need to define 4 util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Improvement 1\n",
    "# the gram matrix of an image tensor (feature-wise outer product) using shifted activations\n",
    "def gram_matrix(x):\n",
    "    assert K.ndim(x) == 3\n",
    "    if K.image_dim_ordering() == \"th\":\n",
    "        features = K.batch_flatten(x)\n",
    "    else:\n",
    "        features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = K.dot(features - 1, K.transpose(features - 1))\n",
    "    return gram\n",
    "\n",
    "\n",
    "# the \"style loss\" is designed to maintain\n",
    "# the style of the reference image in the generated image.\n",
    "# It is based on the gram matrices (which capture style) of\n",
    "# feature maps from the style reference image\n",
    "# and from the generated image\n",
    "def style_loss(style, combination, mask_path=None, nb_channels=None):\n",
    "    assert K.ndim(style) == 3, \"Style matrix must have 3 dimensions. Found {}.\".format(K.ndim(style))\n",
    "    assert K.ndim(combination) == 3, \"Combination matrix must have 3 dimensions. Found {}.\".format(K.ndim(combination))\n",
    "\n",
    "    if mask_path is not None:\n",
    "        style_mask = load_mask(mask_path, nb_channels)\n",
    "\n",
    "        style = style * style_mask\n",
    "        combination = combination * style_mask\n",
    "\n",
    "        del style_mask\n",
    "\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_width * img_height\n",
    "    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))\n",
    "\n",
    "\n",
    "# an auxiliary loss function\n",
    "# designed to maintain the \"content\" of the\n",
    "# base image in the generated image\n",
    "def content_loss(base, combination):\n",
    "    channel_dim = 0 if K.image_dim_ordering() == \"th\" else K.ndim(K.shape(base)) - 1\n",
    "\n",
    "    channels = K.shape(base)[channel_dim]\n",
    "    size = img_width * img_height\n",
    "\n",
    "    if content_loss_type == 1:\n",
    "        multiplier = 1 / (2. * channels ** 0.5 * size ** 0.5)\n",
    "    elif content_loss_type == 2:\n",
    "        multiplier = 1 / (channels * size)\n",
    "    else:\n",
    "        multiplier = 1.\n",
    "\n",
    "    return multiplier * K.sum(K.square(combination - base))\n",
    "\n",
    "\n",
    "# the 3rd loss function, total variation loss,\n",
    "# designed to keep the generated image locally coherent\n",
    "def total_variation_loss(x):\n",
    "    assert K.ndim(x) == 4\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        a = K.square(x[:, :, :img_width - 1, :img_height - 1] - x[:, :, 1:, :img_height - 1])\n",
    "        b = K.square(x[:, :, :img_width - 1, :img_height - 1] - x[:, :, :img_width - 1, 1:])\n",
    "    else:\n",
    "        a = K.square(x[:, :img_width - 1, :img_height - 1, :] - x[:, 1:, :img_height - 1, :])\n",
    "        b = K.square(x[:, :img_width - 1, :img_height - 1, :] - x[:, :img_width - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if vgg_model == \"vgg19\":\n",
    "    feature_layers = ['conv1_1', 'conv1_2', 'conv2_1', 'conv2_2', 'conv3_1', 'conv3_2', 'conv3_3', 'conv3_4',\n",
    "                      'conv4_1', 'conv4_2', 'conv4_3', 'conv4_4', 'conv5_1', 'conv5_2', 'conv5_3', 'conv5_4']\n",
    "else:\n",
    "    feature_layers = ['conv1_1', 'conv1_2', 'conv2_1', 'conv2_2', 'conv3_1', 'conv3_2', 'conv3_3',\n",
    "                      'conv4_1', 'conv4_2', 'conv4_3', 'conv5_1', 'conv5_2', 'conv5_3']\n",
    "\n",
    "# combine these loss functions into a single scalar\n",
    "loss = K.variable(0.)\n",
    "layer_features = outputs_dict[content_layer]\n",
    "base_image_features = layer_features[0, :, :, :]\n",
    "combination_features = layer_features[nb_tensors - 1, :, :, :]\n",
    "loss += content_weight * content_loss(base_image_features,\n",
    "                                      combination_features)\n",
    "# Improvement 2\n",
    "# Use all layers for style feature extraction and reconstruction\n",
    "nb_layers = len(feature_layers) - 1\n",
    "\n",
    "style_masks = []\n",
    "if style_masks_present:\n",
    "    style_masks = mask_paths # If mask present, pass dictionary of masks to style loss\n",
    "else:\n",
    "    style_masks = [None for _ in range(nb_style_images)] # If masks not present, pass None to the style loss\n",
    "\n",
    "channel_index = 1 if K.image_dim_ordering() == \"th\" else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Improvement 3 : Chained Inference without blurring\n",
    "for i in range(nb_layers):\n",
    "    layer_features = outputs_dict[feature_layers[i]]\n",
    "    shape = shape_dict[feature_layers[i]]\n",
    "    combination_features = layer_features[nb_tensors - 1, :, :, :]\n",
    "    style_reference_features = layer_features[1:nb_tensors - 1, :, :, :]\n",
    "    sl1 = []\n",
    "    for j in range(nb_style_images):\n",
    "        ref_feats = layer_features[j+1, :, :, :]\n",
    "        cur_mask = style_masks[j]\n",
    "        sl1.append(style_loss(ref_feats, combination_features, cur_mask, shape))\n",
    "\n",
    "    layer_features = outputs_dict[feature_layers[i + 1]]\n",
    "    shape = shape_dict[feature_layers[i + 1]]\n",
    "    combination_features = layer_features[nb_tensors - 1, :, :, :]\n",
    "    style_reference_features = layer_features[1:nb_tensors - 1, :, :, :]\n",
    "    sl2 = []\n",
    "    for j in range(nb_style_images):\n",
    "        ref_feats = layer_features[j+1, :, :, :]\n",
    "        sl2.append(style_loss(ref_feats, combination_features, style_masks[j], shape))\n",
    "\n",
    "    for j in range(nb_style_images):\n",
    "        sl = sl1[j] - sl2[j]\n",
    "\n",
    "        # Improvement 4\n",
    "        # Geometric weighted scaling of style loss\n",
    "        loss += (style_weights[j] / (2 ** (nb_layers - (i + 1)))) * sl\n",
    "\n",
    "loss += total_variation_weight * total_variation_loss(combination_image)\n",
    "\n",
    "# get the gradients of the generated image wrt the loss\n",
    "grads = K.gradients(loss, combination_image)\n",
    "\n",
    "outputs = [loss]\n",
    "if type(grads) in {list, tuple}:\n",
    "    outputs += grads\n",
    "else:\n",
    "    outputs.append(grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "f_outputs = K.function([combination_image], outputs)\n",
    "\n",
    "\n",
    "def eval_loss_and_grads(x):\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        x = x.reshape((1, 3, img_width, img_height))\n",
    "    else:\n",
    "        x = x.reshape((1, img_width, img_height, 3))\n",
    "    outs = f_outputs([x])\n",
    "    loss_value = outs[0]\n",
    "    if len(outs[1:]) == 1:\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "    else:\n",
    "        grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
    "    return loss_value, grad_values\n",
    "\n",
    "\n",
    "# this Evaluator class makes it possible\n",
    "# to compute loss and gradients in one pass\n",
    "# while retrieving them via two separate functions,\n",
    "# \"loss\" and \"grads\". This is done because scipy.optimize\n",
    "# requires separate functions for loss and gradients,\n",
    "# but computing them separately would be inefficient.\n",
    "class Evaluator(object):\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values\n",
    "\n",
    "\n",
    "evaluator = Evaluator()\n",
    "\n",
    "# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
    "# so as to minimize the neural style loss\n",
    "\n",
    "\n",
    "if \"content\" in init_image or \"gray\" in init_image:\n",
    "    x = preprocess_image(base_image_path, True, read_mode=read_mode)\n",
    "else:\n",
    "    x = np.random.uniform(0, 255, (1, img_width, img_height, 3)) - 128.\n",
    "\n",
    "    if K.image_dim_ordering() == \"th\":\n",
    "        x = x.transpose((0, 3, 1, 2))\n",
    "\n",
    "# We require original image if we are to preserve color in YCbCr mode\n",
    "if preserve_color:\n",
    "    content = imread(base_image_path, mode=\"YCbCr\")\n",
    "    content = imresize(content, (img_width, img_height))\n",
    "\n",
    "    if color_mask is not None:\n",
    "        if K.image_dim_ordering() == \"th\":\n",
    "            color_mask_shape = (None, None, img_width, img_height)\n",
    "        else:\n",
    "            color_mask_shape = (None, img_width, img_height, None)\n",
    "\n",
    "        color_mask = load_mask(color_mask, color_mask_shape, return_mask_img=True)\n",
    "    else:\n",
    "        color_mask = None\n",
    "else:\n",
    "    color_mask = None\n",
    "\n",
    "prev_min_val = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of iteration 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-51aa74a45039>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmin_l_bfgs_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfprime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxfun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprev_min_val\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/miniconda2/lib/python2.7/site-packages/scipy/optimize/lbfgsb.pyc\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0;32m--> 193\u001b[0;31m                            **opts)\n\u001b[0m\u001b[1;32m    194\u001b[0m     d = {'grad': res['jac'],\n\u001b[1;32m    195\u001b[0m          \u001b[0;34m'task'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/miniconda2/lib/python2.7/site-packages/scipy/optimize/lbfgsb.pyc\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/miniconda2/lib/python2.7/site-packages/scipy/optimize/lbfgsb.pyc\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/miniconda2/lib/python2.7/site-packages/scipy/optimize/optimize.pyc\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-bc510ba84c87>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_loss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-bc510ba84c87>\u001b[0m in \u001b[0;36meval_loss_and_grads\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/miniconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0mupdated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/miniconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 372\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    373\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/miniconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 636\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    637\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/miniconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 708\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    709\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/root/miniconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    713\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/miniconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    695\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(num_iter):\n",
    "    print(\"Start of iteration\", (i + 1))\n",
    "    start_time = time.time()\n",
    "\n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(), fprime=evaluator.grads, maxfun=20)\n",
    "\n",
    "    if prev_min_val == -1:\n",
    "        prev_min_val = min_val\n",
    "\n",
    "    improvement = (prev_min_val - min_val) / prev_min_val * 100\n",
    "\n",
    "    print(\"Current loss value:\", min_val, \" Improvement : %0.3f\" % improvement, \"%\")\n",
    "    prev_min_val = min_val\n",
    "    # save current generated image\n",
    "    img = deprocess_image(x.copy())\n",
    "\n",
    "    if preserve_color and content is not None:\n",
    "        img = original_color_transform(content, img, mask=color_mask)\n",
    "\n",
    "    if maintain_aspect_ratio & (not rescale_image):\n",
    "        img_ht = int(img_width * aspect_ratio)\n",
    "        print(\"Rescaling Image to (%d, %d)\" % (img_width, img_ht))\n",
    "        img = imresize(img, (img_width, img_ht), interp=rescale_method)\n",
    "\n",
    "    if rescale_image:\n",
    "        print(\"Rescaling Image to (%d, %d)\" % (img_WIDTH, img_HEIGHT))\n",
    "        img = imresize(img, (img_WIDTH, img_HEIGHT), interp=rescale_method)\n",
    "\n",
    "    fname = result_prefix + \"_at_iteration_%d.png\" % (i + 1)\n",
    "    imsave(fname, img)\n",
    "    end_time = time.time()\n",
    "    print(\"Image saved as\", fname)\n",
    "    print(\"Iteration %d completed in %ds\" % (i + 1, end_time - start_time))\n",
    "\n",
    "    if improvement_threshold is not 0.0:\n",
    "        if improvement < improvement_threshold and improvement is not 0.0:\n",
    "            print(\"Improvement (%f) is less than improvement threshold (%f). Early stopping script.\" %\n",
    "                  (improvement, improvement_threshold))\n",
    "            exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
